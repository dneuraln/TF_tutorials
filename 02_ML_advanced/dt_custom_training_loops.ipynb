{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d306feb-1b54-486a-b214-a4a03f183fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.tensorflow.org/tutorials/distribute/custom_training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37127d7-9e83-49db-8106-a11670aea5ef",
   "metadata": {},
   "source": [
    "``` text\n",
    "This tutorial demonstrates how to use tf.distribute.Strategy with custom training loops. We will train a simple CNN model on the fashion MNIST dataset. The fashion MNIST dataset contains 60000 train images of size 28 x 28 and 10000 test images of size 28 x 28.\n",
    "\n",
    "We are using custom training loops to train our model because they give us flexibility and a greater control on training. Moreover, it is easier to debug the model and the training loop.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2efa6d4b-f58e-4c0b-8570-1a2887231a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0\n"
     ]
    }
   ],
   "source": [
    "# Import TensorFlow\n",
    "import tensorflow as tf\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134c3018-97d0-433a-b853-85df258f602f",
   "metadata": {},
   "source": [
    "# Download the fashion MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "289428b7-cd81-48d3-82ca-dd635da5c95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "\n",
    "# Adding a dimension to the array -> new shape == (28, 28, 1)\n",
    "# We are doing this because the first layer in our model is a convolutional\n",
    "# layer and it requires a 4D input (batch_size, height, width, channels).\n",
    "# batch_size dimension will be added later on.\n",
    "train_images = train_images[..., None] # adding channel dimension for a convolutional layer.\n",
    "test_images = test_images[..., None]\n",
    "\n",
    "# Getting the images in [0, 1] range.\n",
    "train_images = train_images / np.float32(255)\n",
    "test_images = test_images / np.float32(255)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075c1b22-8b53-4fcb-abca-ae5a78cc47e2",
   "metadata": {},
   "source": [
    "## Create a strategy to distribute the variables and the graph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d88c15b-d7a2-49e6-970c-0933040cefa6",
   "metadata": {},
   "source": [
    "How does tf.distribute.MirroredStrategy strategy work?\n",
    "\n",
    "* All the variables and the model graph is replicated on the replicas.\n",
    "* Input is evenly distributed across the replicas.\n",
    "* Each replica calculates the loss and gradients for the input it received.\n",
    "* The gradients are synced across all the replicas by summing them.\n",
    "* After the sync, the same update is made to the copies of the variables on each replica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "279db461-ae58-4464-b413-d3e5c899ff13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-15 11:42:50.987663: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 47220 MB memory:  -> device: 0, name: Quadro RTX 8000, pci bus id: 0000:67:00.0, compute capability: 7.5\n",
      "2021-09-15 11:42:50.989091: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 46874 MB memory:  -> device: 1, name: Quadro RTX 8000, pci bus id: 0000:68:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n"
     ]
    }
   ],
   "source": [
    "# If the list of devices is not specified in the\n",
    "# `tf.distribute.MirroredStrategy` constructor, it will be auto-detected.\n",
    "strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e05504e0-524b-40c7-88d6-098a63c99bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numer of devices: 2\n"
     ]
    }
   ],
   "source": [
    "print('Numer of devices: {}'.format(strategy.num_replicas_in_sync))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7684c681-f9f9-4570-a24b-6aa3595c4b66",
   "metadata": {},
   "source": [
    "## Set up the input pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab1b46f9-caf8-40f8-b6c5-0be4d22d89b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(train_images)\n",
    "\n",
    "BATCH_SIZE_PER_REPLICA = 64\n",
    "GLOBAL_BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\n",
    "\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e0dceb-04f1-486d-8e03-b963e5ba50ab",
   "metadata": {},
   "source": [
    "Create the datasets and distribute them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c3d4637-9328-458a-9bf6-c6f95c2e0a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-15 11:45:37.649095: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:705] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorSliceDataset/_2\"\n",
      "op: \"TensorSliceDataset\"\n",
      "input: \"Placeholder/_0\"\n",
      "input: \"Placeholder/_1\"\n",
      "attr {\n",
      "  key: \"Toutput_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_UINT8\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"is_files\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 28\n",
      "        }\n",
      "        dim {\n",
      "          size: 28\n",
      "        }\n",
      "        dim {\n",
      "          size: 1\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "2021-09-15 11:45:37.670862: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:705] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorSliceDataset/_2\"\n",
      "op: \"TensorSliceDataset\"\n",
      "input: \"Placeholder/_0\"\n",
      "input: \"Placeholder/_1\"\n",
      "attr {\n",
      "  key: \"Toutput_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_UINT8\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"is_files\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 28\n",
      "        }\n",
      "        dim {\n",
      "          size: 28\n",
      "        }\n",
      "        dim {\n",
      "          size: 1\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).shuffle(BUFFER_SIZE).batch(GLOBAL_BATCH_SIZE) \n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(GLOBAL_BATCH_SIZE) \n",
    "\n",
    "train_dist_dataset = strategy.experimental_distribute_dataset(train_dataset)\n",
    "test_dist_dataset = strategy.experimental_distribute_dataset(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0bd5fd-a3aa-441b-b98f-85f6944a0dc7",
   "metadata": {},
   "source": [
    "## Create the model\n",
    "\n",
    "Create a model using tf.keras.Sequential. You can also use the Model Subclassing API to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afabc7ce-756a-48a0-9a9a-cabc0b981f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "  model = tf.keras.Sequential([\n",
    "      tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "      tf.keras.layers.MaxPooling2D(),\n",
    "      tf.keras.layers.Conv2D(64, 3, activation='relu'),\n",
    "      tf.keras.layers.MaxPooling2D(),\n",
    "      tf.keras.layers.Flatten(),\n",
    "      tf.keras.layers.Dense(64, activation='relu'),\n",
    "      tf.keras.layers.Dense(10)\n",
    "    ])\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a540cd2-569b-4a9a-9edf-12e04b6410c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a checkpoint directory to store the checkpoints.\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80518b7-27f8-4f75-890e-73725acb4bf4",
   "metadata": {},
   "source": [
    "## Define the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e7d083a-18ba-4214-a457-1e5903aaa17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "  # Set reduction to `none` so we can do the reduction afterwards and divide by\n",
    "  # global batch size.\n",
    "  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "      from_logits=True,\n",
    "      reduction=tf.keras.losses.Reduction.NONE)\n",
    "  def compute_loss(labels, predictions):\n",
    "    per_example_loss = loss_object(labels, predictions)\n",
    "    return tf.nn.compute_average_loss(per_example_loss, global_batch_size=GLOBAL_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ada65b7-e2e2-403f-ae72-24dff2e33583",
   "metadata": {},
   "source": [
    "## Define the metrics to track loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28f396d4-6f6f-41bc-bbac-249acdb311aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "  test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "\n",
    "  train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "      name='train_accuracy')\n",
    "  test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "      name='test_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53dd1064-1ea4-400e-9a99-10b1a5a01670",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a1bfbc5-fcea-41c3-8ec6-30abf7a36e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model, optimizer, and checkpoint must be created under `strategy.scope`.\n",
    "with strategy.scope():\n",
    "  model = create_model()\n",
    "\n",
    "  optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "  checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d184ec20-3d1d-4a96-ae71-7e06954b4c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(inputs):\n",
    "  images, labels = inputs\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "    predictions = model(images, training=True)\n",
    "    loss = compute_loss(labels, predictions)\n",
    "\n",
    "  gradients = tape.gradient(loss, model.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "  train_accuracy.update_state(labels, predictions)\n",
    "  return loss \n",
    "\n",
    "def test_step(inputs):\n",
    "  images, labels = inputs\n",
    "\n",
    "  predictions = model(images, training=False)\n",
    "  t_loss = loss_object(labels, predictions)\n",
    "\n",
    "  test_loss.update_state(t_loss)\n",
    "  test_accuracy.update_state(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd359b46-7d65-4af9-af81-581ab7ea64ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:batch_all_reduce: 8 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:batch_all_reduce: 8 all-reduces with algorithm = nccl, num_packs = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-15 12:12:43.807069: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8202\n",
      "2021-09-15 12:12:44.798571: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:batch_all_reduce: 8 all-reduces with algorithm = nccl, num_packs = 1\n",
      "Epoch 1, Loss: 0.5525131821632385, Accuracy: 80.1866683959961, Test Loss: 0.4275776445865631, Test Accuracy: 84.41999816894531\n",
      "Epoch 2, Loss: 0.3616889417171478, Accuracy: 87.1383285522461, Test Loss: 0.3542369604110718, Test Accuracy: 87.2699966430664\n",
      "Epoch 3, Loss: 0.31681355834007263, Accuracy: 88.538330078125, Test Loss: 0.3333788812160492, Test Accuracy: 88.13999938964844\n",
      "Epoch 4, Loss: 0.2883014380931854, Accuracy: 89.60833740234375, Test Loss: 0.3262891471385956, Test Accuracy: 88.36000061035156\n",
      "Epoch 5, Loss: 0.26733988523483276, Accuracy: 90.2683334350586, Test Loss: 0.3004698157310486, Test Accuracy: 89.1300048828125\n",
      "Epoch 6, Loss: 0.2474144548177719, Accuracy: 90.94499969482422, Test Loss: 0.30530864000320435, Test Accuracy: 89.0999984741211\n",
      "Epoch 7, Loss: 0.23352253437042236, Accuracy: 91.38166809082031, Test Loss: 0.27879011631011963, Test Accuracy: 89.95000457763672\n",
      "Epoch 8, Loss: 0.21598467230796814, Accuracy: 91.9816665649414, Test Loss: 0.265256404876709, Test Accuracy: 90.19000244140625\n",
      "Epoch 9, Loss: 0.20188108086585999, Accuracy: 92.58499908447266, Test Loss: 0.2636782228946686, Test Accuracy: 90.55000305175781\n",
      "Epoch 10, Loss: 0.19137196242809296, Accuracy: 92.97500610351562, Test Loss: 0.26009759306907654, Test Accuracy: 90.61000061035156\n"
     ]
    }
   ],
   "source": [
    "# `run` replicates the provided computation and runs it\n",
    "# with the distributed input.\n",
    "@tf.function\n",
    "def distributed_train_step(dataset_inputs):\n",
    "  per_replica_losses = strategy.run(train_step, args=(dataset_inputs,))\n",
    "  return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses,\n",
    "                         axis=None)\n",
    "\n",
    "@tf.function\n",
    "def distributed_test_step(dataset_inputs):\n",
    "  return strategy.run(test_step, args=(dataset_inputs,))\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  # TRAIN LOOP\n",
    "  total_loss = 0.0\n",
    "  num_batches = 0\n",
    "  for x in train_dist_dataset:\n",
    "    total_loss += distributed_train_step(x)\n",
    "    num_batches += 1\n",
    "  train_loss = total_loss / num_batches\n",
    "\n",
    "  # TEST LOOP\n",
    "  for x in test_dist_dataset:\n",
    "    distributed_test_step(x)\n",
    "\n",
    "  if epoch % 2 == 0:\n",
    "    checkpoint.save(checkpoint_prefix)\n",
    "\n",
    "  template = (\"Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, \"\n",
    "              \"Test Accuracy: {}\")\n",
    "  print (template.format(epoch+1, train_loss,\n",
    "                         train_accuracy.result()*100, test_loss.result(),\n",
    "                         test_accuracy.result()*100))\n",
    "\n",
    "  test_loss.reset_states()\n",
    "  train_accuracy.reset_states()\n",
    "  test_accuracy.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae0c134-b94c-479e-9f99-98684a692a1b",
   "metadata": {},
   "source": [
    "Things to note in the example above:\n",
    "\n",
    "* We are iterating over the train_dist_dataset and test_dist_dataset using a for x in ... construct.\n",
    "\n",
    "* The scaled loss is the return value of the distributed_train_step. This value is aggregated across replicas using the tf.distribute.Strategy.reduce call and then across batches by summing the return value of the tf.distribute.Strategy.reduce calls.\n",
    "\n",
    "* tf.keras.Metrics should be updated inside train_step and test_step that gets executed by tf.distribute.Strategy.run. *tf.distribute.Strategy.run returns results from each local replica in the strategy, and there are multiple ways to consume this result. You can do tf.distribute.Strategy.reduce to get an aggregated value. You can also do tf.distribute.Strategy.experimental_local_results to get the list of values contained in the result, one per local replica.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af81aa2e-c475-43f3-ba88-5ebbaeb0cca6",
   "metadata": {},
   "source": [
    "## Restore the latest checkpoint and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a70b18c7-9841-4ded-aa5f-d90bdd5b82b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='eval_accuracy')\n",
    "new_model = create_model()\n",
    "new_optimizer = tf.keras.optimizers.Adam()\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(GLOBAL_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dad0bf82-ef81-4237-887c-649fbc2f62db",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def eval_step(images, labels):\n",
    "  predictions = new_model(images, training=False)\n",
    "  eval_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "64015a9a-d1ad-4ab0-9a2c-9d248ba75528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after restoring the saved model without strategy: 90.55000305175781\n"
     ]
    }
   ],
   "source": [
    "checkpoint = tf.train.Checkpoint(optimizer=new_optimizer, model=new_model)\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "for images, labels in test_dataset:\n",
    "  eval_step(images, labels)\n",
    "\n",
    "print ('Accuracy after restoring the saved model without strategy: {}'.format(\n",
    "    eval_accuracy.result()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46faddcb-02e9-4153-bbb0-69cd570a7df8",
   "metadata": {},
   "source": [
    "## Alternate ways of iterating over a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "00f89f61-2729-47f9-aabb-90dff6f00be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 0.16488070785999298, Accuracy: 93.828125\n",
      "Epoch 10, Loss: 0.1774645745754242, Accuracy: 94.0625\n",
      "Epoch 10, Loss: 0.14581023156642914, Accuracy: 94.21875\n",
      "Epoch 10, Loss: 0.15926451981067657, Accuracy: 94.140625\n",
      "Epoch 10, Loss: 0.16859349608421326, Accuracy: 93.515625\n",
      "Epoch 10, Loss: 0.1727086901664734, Accuracy: 94.453125\n",
      "Epoch 10, Loss: 0.17819590866565704, Accuracy: 94.21875\n",
      "Epoch 10, Loss: 0.17876367270946503, Accuracy: 94.0625\n",
      "Epoch 10, Loss: 0.1698833405971527, Accuracy: 94.140625\n",
      "Epoch 10, Loss: 0.18342843651771545, Accuracy: 93.359375\n"
     ]
    }
   ],
   "source": [
    "for _ in range(EPOCHS):\n",
    "  total_loss = 0.0\n",
    "  num_batches = 0\n",
    "  train_iter = iter(train_dist_dataset)\n",
    "\n",
    "  for _ in range(10):\n",
    "    total_loss += distributed_train_step(next(train_iter))\n",
    "    num_batches += 1\n",
    "  average_train_loss = total_loss / num_batches\n",
    "\n",
    "  template = (\"Epoch {}, Loss: {}, Accuracy: {}\")\n",
    "  print (template.format(epoch+1, average_train_loss, train_accuracy.result()*100))\n",
    "  train_accuracy.reset_states()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf358ee1-2f70-4a80-a7f9-b82877e38c74",
   "metadata": {},
   "source": [
    "### Iterating inside a tf.function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf99507-d7be-45ad-a194-53d48276ad7d",
   "metadata": {},
   "source": [
    "```\n",
    "You can also iterate over the entire input train_dist_dataset inside a tf.function using the for x in ... construct or by creating iterators like we did above. The example below demonstrates wrapping one epoch of training in a tf.function and iterating over train_dist_dataset inside the function.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "79de495d-a5c3-41f6-973c-0f4f5378965d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 0.1833042949438095, Accuracy: 94.140625\n",
      "Epoch 10, Loss: 0.16555854678153992, Accuracy: 94.53125\n",
      "Epoch 10, Loss: 0.1660725176334381, Accuracy: 93.828125\n",
      "Epoch 10, Loss: 0.15906807780265808, Accuracy: 93.90625\n",
      "Epoch 10, Loss: 0.15899965167045593, Accuracy: 94.6875\n",
      "Epoch 10, Loss: 0.1722952276468277, Accuracy: 94.53125\n",
      "Epoch 10, Loss: 0.1721753180027008, Accuracy: 93.359375\n",
      "Epoch 10, Loss: 0.16018742322921753, Accuracy: 94.453125\n",
      "Epoch 10, Loss: 0.1922616958618164, Accuracy: 93.046875\n",
      "Epoch 10, Loss: 0.19559378921985626, Accuracy: 92.578125\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).model.layer_with_weights-0.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).model.layer_with_weights-0.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).model.layer_with_weights-1.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).model.layer_with_weights-1.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).model.layer_with_weights-2.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).model.layer_with_weights-2.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).model.layer_with_weights-3.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).model.layer_with_weights-3.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).model.layer_with_weights-0.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).model.layer_with_weights-0.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).model.layer_with_weights-1.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).model.layer_with_weights-1.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).model.layer_with_weights-2.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).model.layer_with_weights-2.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).model.layer_with_weights-3.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).model.layer_with_weights-3.bias\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n"
     ]
    }
   ],
   "source": [
    "for _ in range(EPOCHS):\n",
    "  total_loss = 0.0\n",
    "  num_batches = 0\n",
    "  train_iter = iter(train_dist_dataset)\n",
    "\n",
    "  for _ in range(10):\n",
    "    total_loss += distributed_train_step(next(train_iter))\n",
    "    num_batches += 1\n",
    "  average_train_loss = total_loss / num_batches\n",
    "\n",
    "  template = (\"Epoch {}, Loss: {}, Accuracy: {}\")\n",
    "  print (template.format(epoch+1, average_train_loss, train_accuracy.result()*100))\n",
    "  train_accuracy.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb93847-8bdd-4471-b5a8-dfdc28c3273d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
