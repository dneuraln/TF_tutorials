{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ee7b671-7672-4118-8bd2-383d1c12a7b3",
   "metadata": {},
   "source": [
    "# Training on multiple GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f778f057-08f4-4560-9f66-a4976af76c50",
   "metadata": {},
   "source": [
    "``` text\n",
    "The tf.distribute.Strategy API provides an abstraction for distributing your training across multiple processing units. It allows you to carry out distributed training using existing models and training code with minimal changes.\n",
    "\n",
    "This tutorial demonstrates how to use the tf.distribute.MirroredStrategy to perform in-graph replication with synchronous training on many GPUs on one machine. The strategy essentially copies all of the model's variables to each processor. Then, it uses all-reduce to combine the gradients from all processors, and applies the combined value to all copies of the model.\n",
    "\n",
    "You will use the tf.keras APIs to build the model and Model.fit for training it. (To learn about distributed training with a custom training loop and the MirroredStrategy, check out this tutorial.)\n",
    "\n",
    "MirroredStrategy trains your model on multiple GPUs on a single machine. For synchronous training on many GPUs on multiple workers, use the tf.distribute.MultiWorkerMirroredStrategy with the Keras Model.fit or a custom training loop. For other options, refer to the Distributed training guide.\n",
    "\n",
    "To learn about various other strategies, there is the Distributed training with TensorFlow guide.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea0a983e-0a7b-462b-a0df-cb096839eb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "\n",
    "# Load the TensorBoard notebook extension.\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3feffbbd-8c03-42ed-b40a-2e913c0feaa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "610d2431-f679-4511-82ed-86f3aef0a2ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-15 11:03:56.839905: W tensorflow/core/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"FAILED_PRECONDITION: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Could not resolve host: metadata\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset 11.06 MiB (download: 11.06 MiB, generated: 21.00 MiB, total: 32.06 MiB) to /home/maker/tensorflow_datasets/mnist/3.0.1...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4e0ef71c26341c3b8a27983502ec824",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...:   0%|          | 0/4 [00:00<?, ? file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset mnist downloaded and prepared to /home/maker/tensorflow_datasets/mnist/3.0.1. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-15 11:04:03.861211: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 47220 MB memory:  -> device: 0, name: Quadro RTX 8000, pci bus id: 0000:67:00.0, compute capability: 7.5\n",
      "2021-09-15 11:04:03.863975: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 46893 MB memory:  -> device: 1, name: Quadro RTX 8000, pci bus id: 0000:68:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "datasets, info = tfds.load(name='mnist', with_info=True, as_supervised=True)\n",
    "\n",
    "mnist_train, mnist_test = datasets['train'], datasets['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd206d03-fe7c-48e9-8616-a7a29b646c9e",
   "metadata": {},
   "source": [
    "## Define the distribution strategy\n",
    "\n",
    "``` text \n",
    "Create a MirroredStrategy object. This will handle distribution and provide a context manager (MirroredStrategy.scope) to build your model inside.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e99dcaef-c9e5-459b-b8d7-a2942e6ed93e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd4c6f12-6e3c-44e8-9543-6ea4223f4970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of devices: 2\n"
     ]
    }
   ],
   "source": [
    "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126d1682-ade9-4a44-9214-a81cf95a5305",
   "metadata": {},
   "source": [
    "## Set up the input pipeline\n",
    "\n",
    "When training a model with multiple GPUs, you can use the extra computing power effectively by increasing the batch size. In general, use the largest batch size that fits the GPU memory and tune the learning rate accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2dc722e-5ba7-481a-a369-d14b3325ed19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also do info.splits.total_num_examples to get the total\n",
    "# number of examples in the dataset.\n",
    "\n",
    "num_train_examples = info.splits['train'].num_examples\n",
    "num_test_examples = info.splits['test'].num_examples\n",
    "\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "BATCH_SIZE_PER_REPLICA = 64\n",
    "BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "667a6429-76a7-43ff-b46a-976062fd52e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(image, label):\n",
    "  image = tf.cast(image, tf.float32)\n",
    "  image /= 255\n",
    "\n",
    "  return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02d19324-32a4-48a4-9547-039aa5cb92e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = mnist_train.map(scale).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "eval_dataset = mnist_test.map(scale).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71cef7a4-114d-4b06-a6c4-4101146cd646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 10000\n"
     ]
    }
   ],
   "source": [
    "print (num_train_examples, num_test_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703baece-7ead-468a-9419-972187ca2397",
   "metadata": {},
   "source": [
    "## Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92a2fd7e-fbd5-4e4a-988a-c7cc7c94fb4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "  model = tf.keras.Sequential([\n",
    "      tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),\n",
    "      tf.keras.layers.MaxPooling2D(),\n",
    "      tf.keras.layers.Flatten(),\n",
    "      tf.keras.layers.Dense(64, activation='relu'),\n",
    "      tf.keras.layers.Dense(10)\n",
    "  ])\n",
    "\n",
    "  model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f3a10f-dce8-44c0-99d5-2f71b2309365",
   "metadata": {},
   "source": [
    "## Define the callbacks\n",
    "\n",
    "Define the following tf.keras.callbacks:\n",
    "\n",
    "* tf.keras.callbacks.TensorBoard: writes a log for TensorBoard, which allows you to visualize the graphs.\n",
    "* tf.keras.callbacks.ModelCheckpoint: saves the model at a certain frequency, such as after every epoch.\n",
    "* tf.keras.callbacks.LearningRateScheduler: schedules the learning rate to change after, for example, every epoch/batch.\n",
    "\n",
    "For illustrative purposes, add a custom callback called PrintLR to display the learning rate in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a78abe8d-c1e6-4d17-83e6-160ae2109394",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-15 11:10:10.391642: I tensorflow/core/profiler/lib/profiler_session.cc:110] Profiler session initializing.\n",
      "2021-09-15 11:10:10.391706: I tensorflow/core/profiler/lib/profiler_session.cc:125] Profiler session started.\n",
      "2021-09-15 11:10:10.394080: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1630] Profiler found 2 GPUs\n",
      "2021-09-15 11:10:10.394882: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcupti.so.11.4'; dlerror: libcupti.so.11.4: cannot open shared object file: No such file or directory\n",
      "2021-09-15 11:10:10.395058: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcupti.so'; dlerror: libcupti.so: cannot open shared object file: No such file or directory\n",
      "2021-09-15 11:10:10.395083: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1681] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.\n",
      "2021-09-15 11:10:10.395291: I tensorflow/core/profiler/lib/profiler_session.cc:143] Profiler session tear down.\n",
      "2021-09-15 11:10:10.395334: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1773] function cupti_interface_->Finalize()failed with error CUPTI could not be loaded or symbol could not be found.\n"
     ]
    }
   ],
   "source": [
    "# Define the checkpoint directory to store the checkpoints.\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Define the name of the checkpoint files.\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "# Define a function for decaying the learning rate.\n",
    "# You can define any decay function you need.\n",
    "def decay(epoch):\n",
    "  if epoch < 3:\n",
    "    return 1e-3\n",
    "  elif epoch >= 3 and epoch < 7:\n",
    "    return 1e-4\n",
    "  else:\n",
    "    return 1e-5\n",
    "\n",
    "# Define a callback for printing the learning rate at the end of each epoch.\n",
    "class PrintLR(tf.keras.callbacks.Callback):\n",
    "  def on_epoch_end(self, epoch, logs=None):\n",
    "    print('\\nLearning rate for epoch {} is {}'.format(epoch + 1,\n",
    "                                                      model.optimizer.lr.numpy()))\n",
    "    \n",
    "# Put all the callbacks together.\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,\n",
    "                                       save_weights_only=True),\n",
    "    tf.keras.callbacks.LearningRateScheduler(decay),\n",
    "    PrintLR()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e43b82e5-5f17-40b3-a93f-a471dafb7913",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-15 11:10:27.276717: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:466] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "INFO:tensorflow:batch_all_reduce: 6 all-reduces with algorithm = nccl, num_packs = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:batch_all_reduce: 6 all-reduces with algorithm = nccl, num_packs = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:batch_all_reduce: 6 all-reduces with algorithm = nccl, num_packs = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:batch_all_reduce: 6 all-reduces with algorithm = nccl, num_packs = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2021-09-15 11:10:32.780614: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8202\n",
      "2021-09-15 11:10:33.425817: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3/469 [..............................] - ETA: 33s - loss: 2.2444 - accuracy: 0.1901   WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0062s vs `on_train_batch_end` time: 0.0220s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-15 11:10:37.958733: I tensorflow/core/profiler/lib/profiler_session.cc:110] Profiler session initializing.\n",
      "2021-09-15 11:10:37.958759: I tensorflow/core/profiler/lib/profiler_session.cc:125] Profiler session started.\n",
      "2021-09-15 11:10:37.958795: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1681] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.\n",
      "2021-09-15 11:10:37.971328: I tensorflow/core/profiler/lib/profiler_session.cc:67] Profiler session collecting data.\n",
      "2021-09-15 11:10:37.971652: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1773] function cupti_interface_->Finalize()failed with error CUPTI could not be loaded or symbol could not be found.\n",
      "2021-09-15 11:10:37.988446: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:526]  GpuTracer has collected 0 callback api events and 0 activity events. \n",
      "2021-09-15 11:10:37.994946: I tensorflow/core/profiler/lib/profiler_session.cc:143] Profiler session tear down.\n",
      "2021-09-15 11:10:38.024944: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: ./logs/plugins/profile/2021_09_15_11_10_37\n",
      "\n",
      "2021-09-15 11:10:38.036951: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to ./logs/plugins/profile/2021_09_15_11_10_37/Developer.trace.json.gz\n",
      "2021-09-15 11:10:38.047640: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: ./logs/plugins/profile/2021_09_15_11_10_37\n",
      "\n",
      "2021-09-15 11:10:38.049173: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to ./logs/plugins/profile/2021_09_15_11_10_37/Developer.memory_profile.json.gz\n",
      "2021-09-15 11:10:38.049425: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: ./logs/plugins/profile/2021_09_15_11_10_37\n",
      "Dumped tool data for xplane.pb to ./logs/plugins/profile/2021_09_15_11_10_37/Developer.xplane.pb\n",
      "Dumped tool data for overview_page.pb to ./logs/plugins/profile/2021_09_15_11_10_37/Developer.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to ./logs/plugins/profile/2021_09_15_11_10_37/Developer.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to ./logs/plugins/profile/2021_09_15_11_10_37/Developer.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to ./logs/plugins/profile/2021_09_15_11_10_37/Developer.kernel_stats.pb\n",
      "\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0062s vs `on_train_batch_end` time: 0.0220s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "469/469 [==============================] - 14s 8ms/step - loss: 0.2622 - accuracy: 0.9266\n",
      "\n",
      "Learning rate for epoch 1 is 0.0010000000474974513\n",
      "Epoch 2/12\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 0.0894 - accuracy: 0.9742\n",
      "\n",
      "Learning rate for epoch 2 is 0.0010000000474974513\n",
      "Epoch 3/12\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 0.0622 - accuracy: 0.9819\n",
      "\n",
      "Learning rate for epoch 3 is 0.0010000000474974513\n",
      "Epoch 4/12\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 0.0400 - accuracy: 0.9891\n",
      "\n",
      "Learning rate for epoch 4 is 9.999999747378752e-05\n",
      "Epoch 5/12\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 0.0372 - accuracy: 0.9896\n",
      "\n",
      "Learning rate for epoch 5 is 9.999999747378752e-05\n",
      "Epoch 6/12\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 0.0353 - accuracy: 0.9906\n",
      "\n",
      "Learning rate for epoch 6 is 9.999999747378752e-05\n",
      "Epoch 7/12\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 0.0336 - accuracy: 0.9911\n",
      "\n",
      "Learning rate for epoch 7 is 9.999999747378752e-05\n",
      "Epoch 8/12\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 0.0311 - accuracy: 0.9919\n",
      "\n",
      "Learning rate for epoch 8 is 9.999999747378752e-06\n",
      "Epoch 9/12\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 0.0308 - accuracy: 0.9922\n",
      "\n",
      "Learning rate for epoch 9 is 9.999999747378752e-06\n",
      "Epoch 10/12\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 0.0306 - accuracy: 0.9922\n",
      "\n",
      "Learning rate for epoch 10 is 9.999999747378752e-06\n",
      "Epoch 11/12\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 0.0304 - accuracy: 0.9923\n",
      "\n",
      "Learning rate for epoch 11 is 9.999999747378752e-06\n",
      "Epoch 12/12\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 0.0302 - accuracy: 0.9922\n",
      "\n",
      "Learning rate for epoch 12 is 9.999999747378752e-06\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6aec13bca0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Train and evaluate\n",
    "\n",
    "EPOCHS = 12\n",
    "\n",
    "model.fit(train_dataset, epochs=EPOCHS, callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7057767b-8bd4-419b-a27d-06b77d836765",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3647772983.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_40459/3647772983.py\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    ls {checkpoint_dir}\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Check the checkpoint directory... run this on your shell \n",
    "ls {checkpoint_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "855b6231-45ee-462d-92ec-7c6ac5d149f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-15 11:12:11.166267: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:466] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================] - 2s 6ms/step - loss: 0.0441 - accuracy: 0.9852\n",
      "Eval loss: 0.044124308973550797, Eval accuracy: 0.9851999878883362\n"
     ]
    }
   ],
   "source": [
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "eval_loss, eval_acc = model.evaluate(eval_dataset)\n",
    "\n",
    "print('Eval loss: {}, Eval accuracy: {}'.format(eval_loss, eval_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d784fc5-da9b-4b87-918b-9819f459af41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-4810643adad454f1\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-4810643adad454f1\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir=logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0b6d14-c619-41f2-bd2c-b78c821a5fc4",
   "metadata": {},
   "source": [
    "## Export to SavedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d32ae36b-20f7-4b72-b507-3e07334fad66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-15 11:14:38.044196: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/assets\n"
     ]
    }
   ],
   "source": [
    "path = 'saved_model/'\n",
    "model.save(path, save_format='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a065265-b259-49f1-bd60-74bda47c89bb",
   "metadata": {},
   "source": [
    "now, load the model without Stategy.scope:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c1400527-5246-407c-a950-2931be10f4f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================] - 0s 3ms/step - loss: 0.0441 - accuracy: 0.9852\n",
      "Eval loss: 0.044124308973550797, Eval Accuracy: 0.9851999878883362\n"
     ]
    }
   ],
   "source": [
    "unreplicated_model = tf.keras.models.load_model(path)\n",
    "\n",
    "unreplicated_model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "eval_loss, eval_acc = unreplicated_model.evaluate(eval_dataset)\n",
    "\n",
    "print('Eval loss: {}, Eval Accuracy: {}'.format(eval_loss, eval_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad48b5b-48d4-4758-be4f-63ea772466bc",
   "metadata": {},
   "source": [
    "Load the model with Strategy.scope:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "408ec274-5ed9-4613-a865-0968e9b4f97f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-15 11:16:28.883440: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:466] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================] - 3s 5ms/step - loss: 0.0441 - accuracy: 0.9852\n",
      "Eval loss: 0.044124308973550797, Eval Accuracy: 0.9851999878883362\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "  replicated_model = tf.keras.models.load_model(path)\n",
    "  replicated_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                           optimizer=tf.keras.optimizers.Adam(),\n",
    "                           metrics=['accuracy'])\n",
    "\n",
    "  eval_loss, eval_acc = replicated_model.evaluate(eval_dataset)\n",
    "  print ('Eval loss: {}, Eval Accuracy: {}'.format(eval_loss, eval_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e23cac-02b5-41e9-b3a7-45c37ad8ff2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
